{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81855b98-1531-42d9-a952-f42c27749ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/02/19 17:21:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#Spark connection with S3 options\n",
    "import os\n",
    "import socket\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Замените следующие значения на свои credentials\n",
    "aws_access_key = \"j612yzOjcIwYyPhVD14P\"\n",
    "aws_secret_key = \"bVTEwJ7sJ0TboubM5wJcn2ieYrV91uXk3N9dJ457\"\n",
    "s3_bucket = \"startde-datasets\"\n",
    "s3_endpoint_url = \"https://s3.lab.karpov.courses\"\n",
    " \n",
    "APACHE_MASTER_IP = socket.gethostbyname(\"apache-spark-master-0.apache-spark-headless.apache-spark.svc.cluster.local\")\n",
    "APACHE_MASTER_URL = f\"spark://{APACHE_MASTER_IP}:7077\"\n",
    "POD_IP = os.environ[\"MY_POD_IP\"]\n",
    "SPARK_APP_NAME = f\"spark-{os.environ['HOSTNAME']}\"\n",
    "JARS = \"\"\"/nfs/env/lib/python3.8/site-packages/pyspark/jars/clickhouse-native-jdbc-shaded-2.6.5.jar, \n",
    "/nfs/env/lib/python3.8/site-packages/pyspark/jars/hadoop-aws-3.3.4.jar,\n",
    "/nfs/env/lib/python3.8/site-packages/pyspark/jars/aws-java-sdk-bundle-1.12.433.jar,\n",
    "/nfs/env/lib/python3.8/site-packages/pyspark/jars/postgresql-42.7.4.jar\n",
    "\"\"\"\n",
    "\n",
    "MEM = \"512m\"\n",
    "CORES = 1\n",
    " \n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(SPARK_APP_NAME).\\\n",
    "        master(APACHE_MASTER_URL).\\\n",
    "        config(\"spark.executor.memory\", MEM).\\\n",
    "        config(\"spark.jars\", JARS).\\\n",
    "        config(\"spark.executor.cores\", CORES).\\\n",
    "        config(\"spark.driver.host\", POD_IP).\\\n",
    "        config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key). \\\n",
    "        config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_key). \\\n",
    "        config(\"fs.s3a.endpoint\", s3_endpoint_url).  \\\n",
    "        config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"). \\\n",
    "        config(\"spark.hadoop.fs.s3a.committer.name\", \"directory\"). \\\n",
    "        config(\"spark.hadoop.fs.s3a.path.style.access\", True). \\\n",
    "        config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"). \\\n",
    "        config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\"). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f07c3d-6ce1-433c-98f5-c1ba5c1207b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/02/19 17:21:56 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|   15192|2013-10-29 00:00:00|                2|PENDING_PAYMENT|\n",
      "|   17253|2013-11-09 00:00:00|                4|PENDING_PAYMENT|\n",
      "|   45832|2014-05-05 00:00:00|                5|PENDING_PAYMENT|\n",
      "|   22457|2013-12-10 00:00:00|                6|PENDING_PAYMENT|\n",
      "|   32895|2014-02-13 00:00:00|                6|PENDING_PAYMENT|\n",
      "|   28539|2014-01-17 00:00:00|                7|PENDING_PAYMENT|\n",
      "|   26052|2014-01-02 00:00:00|                7|PENDING_PAYMENT|\n",
      "|   65018|2014-04-27 00:00:00|                8|PENDING_PAYMENT|\n",
      "|   62064|2014-01-08 00:00:00|                8|PENDING_PAYMENT|\n",
      "|   67610|2013-09-12 00:00:00|                9|PENDING_PAYMENT|\n",
      "|   64753|2014-04-17 00:00:00|                9|PENDING_PAYMENT|\n",
      "|   18918|2013-11-19 00:00:00|                9|PENDING_PAYMENT|\n",
      "|   42837|2014-04-16 00:00:00|               11|PENDING_PAYMENT|\n",
      "|   20537|2013-11-29 00:00:00|               12|PENDING_PAYMENT|\n",
      "|     921|2013-07-30 00:00:00|               12|PENDING_PAYMENT|\n",
      "|   36246|2014-03-04 00:00:00|               12|PENDING_PAYMENT|\n",
      "|   45831|2014-05-05 00:00:00|               12|PENDING_PAYMENT|\n",
      "|   66343|2014-06-15 00:00:00|               14|PENDING_PAYMENT|\n",
      "|    8252|2013-09-14 00:00:00|               15|PENDING_PAYMENT|\n",
      "|   19211|2013-11-21 00:00:00|               16|PENDING_PAYMENT|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#В переменную student_directory необходимо подставить свой username в karpov.courses\n",
    "student_directory = 'marija-shkurat-wrn7887'\n",
    "\n",
    "#Читаем данные из parquet\n",
    "df_orders_parquet = spark.read.parquet(f\"s3a://{student_directory}/orders_filtered/\")\n",
    "df_orders_parquet.sort(col(\"order_customer_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd58ea70-d164-454d-a5b3-4379983dbaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|order_customer_id|order_count|\n",
      "+-----------------+-----------+\n",
      "|            11858|          3|\n",
      "|             8389|          2|\n",
      "|             6658|          4|\n",
      "|             9465|          2|\n",
      "|             3175|          2|\n",
      "|              833|          2|\n",
      "|            10817|          2|\n",
      "|            11141|          2|\n",
      "|             4900|          2|\n",
      "|             4818|          3|\n",
      "|             5803|          2|\n",
      "|            10623|          3|\n",
      "|             6620|          3|\n",
      "|             6466|          3|\n",
      "|             8086|          2|\n",
      "|             9427|          3|\n",
      "|             5156|          2|\n",
      "|             2659|          2|\n",
      "|             3794|          2|\n",
      "|            11317|          2|\n",
      "+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4221"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "orders_filtered = df_orders_parquet. \\\n",
    "    groupBy('order_customer_id').agg(count('order_customer_id').alias('order_count')). \\\n",
    "    filter(col('order_count') > 1)\n",
    "orders_filtered.show()\n",
    "orders_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db003a2a-39f9-4b2b-8b30-43ebb0a51bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/02/19 17:22:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "#Spark connection\n",
    "import os\n",
    "import socket\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "APACHE_MASTER_IP = socket.gethostbyname(\"apache-spark-master-0.apache-spark-headless.apache-spark.svc.cluster.local\")\n",
    "APACHE_MASTER_URL = f\"spark://{APACHE_MASTER_IP}:7077\"\n",
    "POD_IP = os.environ[\"MY_POD_IP\"]\n",
    "SPARK_APP_NAME = f\"spark-{os.environ['HOSTNAME']}\"\n",
    "#JARS = \"/nfs/env/lib/python3.8/site-packages/pyspark/jars/clickhouse-native-jdbc-shaded-2.6.5.jar\"\n",
    "JARS = \"/nfs/env/lib/python3.8/site-packages/pyspark/jars/postgresql-42.7.4.jar\"\n",
    "MEM = \"512m\"\n",
    "CORES = 1\n",
    " \n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(SPARK_APP_NAME).\\\n",
    "        master(APACHE_MASTER_URL).\\\n",
    "        config(\"spark.executor.memory\", MEM).\\\n",
    "        config(\"spark.jars\", JARS).\\\n",
    "        config(\"spark.executor.cores\", CORES).\\\n",
    "        config(\"spark.driver.host\", POD_IP).\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a4b88bf-ae1c-4d18-95eb-414ba52046cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|order_customer_id|order_count|\n",
      "+-----------------+-----------+\n",
      "|            11858|          3|\n",
      "|             8389|          2|\n",
      "|             6658|          4|\n",
      "|             9465|          2|\n",
      "|             3175|          2|\n",
      "|              833|          2|\n",
      "|            10817|          2|\n",
      "|            11141|          2|\n",
      "|             4900|          2|\n",
      "|             4818|          3|\n",
      "|             5803|          2|\n",
      "|            10623|          3|\n",
      "|             6620|          3|\n",
      "|             6466|          3|\n",
      "|             8086|          2|\n",
      "|             9427|          3|\n",
      "|             5156|          2|\n",
      "|             2659|          2|\n",
      "|             3794|          2|\n",
      "|            11317|          2|\n",
      "+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4221"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_filtered.show()\n",
    "orders_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf3ad38f-e533-43b9-b310-ccfb97459176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "URL = \"jdbc:postgresql://startde.postgres.karpov.courses:5432/demo\"\n",
    "TABLE = \"marija_shkurat_wrn7887.orders_filtered\"\n",
    "USERNAME = \"marija_shkurat_wrn7887\"\n",
    "PASSWORD = \"c529eb9ae72afcb5783f6ffc20962269ba00d142f0aec49d150a7704a153e929\"\n",
    "DRIVER = \"org.postgresql.Driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb9e395-31d6-42ce-9f04-091eb6661948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Базовая запись в PostgreSQL\n",
    "orders_filtered.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", URL) \\\n",
    "  .option(\"dbtable\", TABLE) \\\n",
    "  .option(\"user\", USERNAME) \\\n",
    "  .option(\"password\", PASSWORD) \\\n",
    "  .option(\"driver\", DRIVER) \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd175f1f-25f3-4b55-af6d-144251c49efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|order_customer_id|order_count|\n",
      "+-----------------+-----------+\n",
      "|            11858|          3|\n",
      "|             8389|          2|\n",
      "|             6658|          4|\n",
      "|             9465|          2|\n",
      "|             3175|          2|\n",
      "|              833|          2|\n",
      "|            10817|          2|\n",
      "|            11141|          2|\n",
      "|             4900|          2|\n",
      "|             4818|          3|\n",
      "|             5803|          2|\n",
      "|            10623|          3|\n",
      "|             6620|          3|\n",
      "|             6466|          3|\n",
      "|             8086|          2|\n",
      "|             9427|          3|\n",
      "|             5156|          2|\n",
      "|             2659|          2|\n",
      "|             3794|          2|\n",
      "|            11317|          2|\n",
      "+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Базовое чтение данных из PostgreSQL\n",
    "orders_filtered = spark.read \\\n",
    "   .format(\"jdbc\") \\\n",
    "   .option(\"url\", URL) \\\n",
    "   .option(\"dbtable\", TABLE) \\\n",
    "   .option(\"user\", USERNAME) \\\n",
    "   .option(\"password\", PASSWORD) \\\n",
    "   .option(\"driver\", DRIVER) \\\n",
    "   .load()\n",
    "\n",
    "orders_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "817c20c0-fba1-4023-af17-f76813fda19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687b00f-f03f-4ea3-9b90-351ce73e1f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
